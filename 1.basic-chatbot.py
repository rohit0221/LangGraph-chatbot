from dotenv import load_dotenv
import os
# Load environment variables
load_dotenv()

# Import Libraries
from langchain_openai import ChatOpenAI

from typing import Annotated
from typing_extensions import TypedDict

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from IPython.display import Image, display



# Initialize the ChatOpenAI model
llm = ChatOpenAI(model="gpt-3.5-turbo")

# Create the State Class
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them
class State(TypedDict):
    messages: Annotated[list, add_messages]

#Class to manage ChatBot. Takes LLM as parameter
class ChatbotManager:
    def __init__(self, model):
        #Assign the model to an instance variable called llm
        self.llm = model


        #Create an instance of Class StateGraph with an initial state called State.
        self.graph_builder = StateGraph(State)
        # Add a node named “chatbot” to the state graph, associated with a function called self.chatbot
        self.graph_builder.add_node("chatbot", self.chatbot)
        # Set the entry point of the state graph to the “chatbot” node.
        # This tells our graph where to start its work each time we run it.
        self.graph_builder.set_entry_point("chatbot")
        # Set the finish point (termination point) of the state graph to the same “chatbot” node. 
        # This instructs the graph "any time this node is run, you can exit."
        self.graph_builder.set_finish_point("chatbot")

        #Compile the state graph into a usable form (e.g., a directed acyclic graph).
        self.graph = self.graph_builder.compile()

# Define the node now. Node is just a function that does some work.  Nodes represent units of work. They are typically regular python functions.  
# in this case the node is a Chatbot that answers the questions.
# Every node node we define will receive the current State as input and return a value that updates that state
    # This fun ction takes input parameter "state" of type "State".
    # processes the current state and generates a response
    # state["messages"] represents the input messages or conversation history stored in the state object.
    # The method invokes the language model with the input messages and returns a dictionary with a single key-value pair:
    # Key: "messages"
    # Value: A list containing the chatbot’s response(s).

    def chatbot(self, state: State):
        return {"messages": [self.llm.invoke(state["messages"])]}

    def display_graph(self):
        try:
            display(Image(self.graph.get_graph().draw_mermaid_png()))
        except Exception as e:
            print(f"Graph display error: {e}")

    def run(self):
        while True:
            user_input = input("User: ")
            if user_input.lower() in ["quit", "exit", "q"]:
                print("Goodbye!")
                break
            #iterate over events from the state graph.
            #The {"messages": ("user", user_input)} dictionary represents the current state with the user’s input message.
            for event in self.graph.stream({"messages": ("user", user_input)}):
                # This inner loop extracts values from the event
                for value in event.values():
                    # retrieve the content of the last message generated by the chatbot.
                    print("Assistant:", value["messages"][-1].content)

if __name__ == "__main__":
    chatbot_manager = ChatbotManager(llm)
    chatbot_manager.display_graph()
    chatbot_manager.run()
